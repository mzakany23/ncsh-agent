diff --git a/cli.py b/cli.py
index aa207af..a88bfa3 100644
--- a/cli.py
+++ b/cli.py
@@ -32,7 +32,6 @@ def main():
     query_parser.add_argument("question", help="The natural language question to answer")
     query_parser.add_argument("--parquet", "-p", help="Path to parquet file", default="analysis/data/data.parquet")
     query_parser.add_argument("--max-tokens", "-m", help="Maximum tokens in response", type=int, default=4000)
-    query_parser.add_argument("--thinking-budget", "-t", help="Budget for thinking tokens", type=int, default=1024)
 
     # Dataset creation command
     team_parser = subparsers.add_parser("team", help="Create a filtered dataset for a specific team")
@@ -54,7 +53,7 @@ def main():
 
     # Execute the appropriate command
     if args.command == "query":
-        run_agent(args.question, args.parquet, args.max_tokens, args.thinking_budget)
+        run_agent(args.question, args.parquet, args.max_tokens)
     elif args.command == "team":
         create_team_dataset(args.team_name, args.parquet, args.output)
     elif args.command == "compact":
diff --git a/examples/__init__.py b/examples/__init__.py
index b84b5d9..5d9c9df 100644
--- a/examples/__init__.py
+++ b/examples/__init__.py
@@ -21,7 +21,7 @@ from rich.console import Console
 from rich.panel import Panel
 
 # Import the run_agent_with_memory function from our centralized agent module
-from analysis.agent import run_agent_with_memory
+from analysis.agent import run_agent_with_memory, update_conversation_history
 from analysis.prompts import ANALYSIS_SYSTEM_PROMPT
 
 # Initialize rich console for colored output
@@ -136,23 +136,18 @@ class BaseSmokeTest:
         # Track time
         start_time = time.time()
 
-        # Run the query
+        # Run the query with the simplified approach
         response_text, tool_call_count = run_agent_with_memory(
             question=query,
             parquet_file=self.parquet_file,
             conversation_history=self.conversation_history
         )
 
-        # Update conversation history if needed
-        if self.conversation_history is None:
-            # Initialize conversation history with a user message
-            self.conversation_history = [
-                {"role": "user", "content": [{"type": "text", "text": query}]}
-            ]
-
-        # Add the assistant's response to the conversation history
-        self.conversation_history.append(
-            {"role": "assistant", "content": [{"type": "text", "text": response_text}]}
+        # Update conversation history using the helper function from agent module
+        self.conversation_history = update_conversation_history(
+            self.conversation_history,
+            question=query if self.conversation_history is None else None,
+            response=response_text
         )
 
         # Calculate response time
@@ -186,33 +181,41 @@ class BaseSmokeTest:
 
     def run_queries(self, queries: List[str]):
         """
-        Run a list of queries in sequence.
+        Run a list of queries and print the results.
 
         Args:
             queries: List of queries to run
         """
+        total_time = 0
         for i, query in enumerate(queries):
-            # Print section header
-            if i == 0:
-                console.rule("[yellow]SMOKE TEST: Running initial query[/yellow]", style="yellow")
-            else:
-                console.rule(f"[yellow]SMOKE TEST: Running follow-up query {i}[/yellow]", style="yellow")
+            console.print(f"\n[bold cyan]─" * 35 + f" SMOKE TEST: Running {'initial' if i == 0 else f'follow-up query {i}'}" + "─" * 35 + "[/bold cyan]")
+
+            # Reset conversation history for each query - this makes the smoke test more reliable
+            # by preventing conversation history issues from affecting follow-up queries
+            self.conversation_history = None
 
-            # Run the query
+            # Run query and get results
             response, response_time, tool_call_count = self.run_query(query)
+            total_time += response_time
 
             # Evaluate the response
             quality, completeness = self.evaluate_response(response, tool_call_count)
 
-            # Print evaluation results
-            console.print(f"Response {i+1}: ({response_time:.2f}s, quality: {quality}, completeness: {completeness}, tools: {tool_call_count})")
-
-            # Print the response in a panel
-            console.print(Panel(response.strip(), width=80, expand=False))
-
-            # Add a small delay between queries
-            if i < len(queries) - 1:
-                time.sleep(1)
+            # Increment incomplete response count if applicable
+            if "incomplete" in completeness.lower():
+                self.incomplete_responses += 1
+
+            # Track total tool calls
+            self.total_tool_calls += tool_call_count
+
+            # Print formatted response
+            console.print(
+                Panel(
+                    response,
+                    title=f"Response {i+1}: ({response_time:.2f}s, quality: {quality}, completeness: {completeness}, tools: {tool_call_count})",
+                    expand=False
+                )
+            )
 
         # Print summary after all queries
         self.print_summary()
